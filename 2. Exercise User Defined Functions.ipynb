{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "054eaef5-1341-4f8b-b5a8-b856d67905c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exercise User Defined Functions\n",
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "\n",
    "In this exercise, we're doing ETL. Instead of using built-in functions, use UDFs to complete the exercise. \n",
    "\n",
    "As a reminder, that file contains data about people, including:\n",
    "\n",
    "\n",
    "* first, middle and last names\n",
    "* gender\n",
    "* birth date\n",
    "* Social Security number\n",
    "* salary\n",
    "\n",
    "But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n",
    "\n",
    "* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\"). \n",
    "* The Social Security numbers aren't consistent, either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n",
    "\n",
    "The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. (The salaries will match, as well,\n",
    "and the Social Security Numbers *would* match, if they were somehow put in the same format).\n",
    "\n",
    "Your job is to remove the duplicate records. The specific requirements of your job are:\n",
    "\n",
    "* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n",
    "* Preserve the data format of the columns. For example, if you write the first name column in all lower-case, you haven't met this requirement.\n",
    "* Write the result as a Parquet file, as designated by *destFile*.\n",
    "* The final Parquet \"file\" must contain 8 part files (8 files ending in \".parquet\").\n",
    "\n",
    "<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** The initial dataset contains 103,000 records.<br/>\n",
    "The de-duplicated result haves 100,000 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75efe262-2524-4cbe-a19e-ce8595fb4811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f117dd0e-def6-488c-be47-903f6c7f9e81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We need UDFs to standardize the case of the name fields and to format SSNs consistently.\n",
    "\n",
    "Case-insensitive name matching: We’ll write UDFs to convert names to lowercase for comparison purposes but keep the original format.\n",
    "SSN formatting: Write a UDF that ensures all SSNs are hyphenated.\n",
    "\n",
    "### **Block 1: Start the Spark Session**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "```\n",
    "- **`from pyspark.sql import SparkSession`**: We import `SparkSession` to create an entry point for working with Spark.\n",
    "- **`from pyspark.sql.functions import udf`**: We import the `udf` function from PySpark to create User Defined Functions.\n",
    "- **`from pyspark.sql.types import StringType`**: We import `StringType` to specify the return type of our UDF (which is string data).\n",
    "- **`import re`**: We import the `re` module, which helps us use regular expressions to reformat Social Security numbers.\n",
    "\n",
    "### **Block 2: Create a Spark Session and Load the Data**\n",
    "```python\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"ETL-Exercise\").getOrCreate()\n",
    "\n",
    "# Load the data\n",
    "data = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "- **`spark = SparkSession.builder.appName(\"ETL-Exercise\").getOrCreate()`**: We create a Spark session. This session allows us to use Spark functions in our program. The `appName` gives our session a name for reference, and `getOrCreate()` either creates a new session or returns an existing one.\n",
    "  \n",
    "- **`data = spark.read.csv(\"path_to_file.csv\", header=True, inferSchema=True)`**: This line loads a CSV file into a DataFrame. \n",
    "    - `path_to_file.csv` is the file’s location.\n",
    "    - `header=True` means that the first row of the CSV contains the column names.\n",
    "    - `inferSchema=True` tells Spark to automatically guess the correct data types for each column.\n",
    "\n",
    "### **Block 3: Define UDFs to Normalize Data**\n",
    "#### UDF 1: Convert names to lowercase\n",
    "```python\n",
    "# Define UDFs for lowercasing names\n",
    "def lowercase_name(name):\n",
    "    return name.lower() if name else None\n",
    "\n",
    "lowercaseUDF = udf(lowercase_name, StringType())\n",
    "```\n",
    "- **`def lowercase_name(name):`**: This defines a function named `lowercase_name` that takes a name as input.\n",
    "- **`return name.lower() if name else None`**: Inside the function, we check if the `name` is not `None`. If a name exists, we convert it to lowercase using `lower()` and return it. If the name is missing (`None`), we return `None`.\n",
    "  \n",
    "- **`lowercaseUDF = udf(lowercase_name, StringType())`**: This registers the `lowercase_name` function as a User Defined Function (UDF) so that we can use it on a Spark DataFrame. We specify that this UDF will return a string (`StringType()`).\n",
    "\n",
    "#### UDF 2: Format SSNs with hyphens\n",
    "```python\n",
    "# Define UDF to format SSNs consistently\n",
    "def format_ssn(ssn):\n",
    "    if ssn:\n",
    "        return re.sub(r\"(\\d{3})(\\d{2})(\\d{4})\", r\"\\1-\\2-\\3\", ssn.replace(\"-\", \"\"))\n",
    "    return None\n",
    "\n",
    "formatSSNUDF = udf(format_ssn, StringType())\n",
    "```\n",
    "- **`def format_ssn(ssn):`**: This defines a function named `format_ssn` to format Social Security Numbers.\n",
    "- **`if ssn:`**: This checks if the SSN exists (is not `None`).\n",
    "  \n",
    "- **`ssn.replace(\"-\", \"\")`**: This removes any existing hyphens from the SSN, making it easier to reformat.\n",
    "  \n",
    "- **`re.sub(r\"(\\d{3})(\\d{2})(\\d{4})\", r\"\\1-\\2-\\3\", ssn)`**: This uses a regular expression (`re.sub`) to ensure that the SSN is formatted as `XXX-XX-XXXX`. It groups the first 3 digits, the next 2 digits, and the last 4 digits, and inserts hyphens in between.\n",
    "  \n",
    "- **`return None`**: If no SSN is provided, the function returns `None`.\n",
    "  \n",
    "- **`formatSSNUDF = udf(format_ssn, StringType())`**: This registers the `format_ssn` function as a UDF so we can apply it to a Spark DataFrame. It will return a formatted string.\n",
    "\n",
    "### **Block 4: Apply UDFs to the DataFrame**\n",
    "```python\n",
    "# Apply UDFs to create new columns\n",
    "data = data.withColumn(\"first_name_lower\", lowercaseUDF(data['first_name'])) \\\n",
    "           .withColumn(\"middle_name_lower\", lowercaseUDF(data['middle_name'])) \\\n",
    "           .withColumn(\"last_name_lower\", lowercaseUDF(data['last_name'])) \\\n",
    "           .withColumn(\"formatted_ssn\", formatSSNUDF(data['ssn']))\n",
    "```\n",
    "- **`withColumn(\"new_column\", someFunction(df['column']))`**: This method creates a new column in the DataFrame. The first argument is the new column’s name, and the second argument is the operation applied to the existing column.\n",
    "  \n",
    "- **`lowercaseUDF(data['first_name'])`**: This applies the `lowercaseUDF` to the `first_name` column, creating a new column called `first_name_lower`. The same process is repeated for `middle_name` and `last_name`.\n",
    "  \n",
    "- **`formatSSNUDF(data['ssn'])`**: This applies the `formatSSNUDF` to the `ssn` column, creating a new column called `formatted_ssn`.\n",
    "\n",
    "### **Block 5: Remove Duplicates**\n",
    "```python\n",
    "# Deduplicate the data\n",
    "deduplicated_data = data.dropDuplicates([\"first_name_lower\", \"middle_name_lower\", \"last_name_lower\", \"birth_date\"])\n",
    "```\n",
    "- **`dropDuplicates([\"columns\"])`**: This method removes rows that are duplicated based on the specified columns. Here, duplicates are removed based on the lowercase versions of the first, middle, and last names, and the `birth_date`. This ensures that names are compared case-insensitively, but only one record is kept.\n",
    "\n",
    "### **Block 6: Clean Up Temporary Columns and Fix SSN Column**\n",
    "```python\n",
    "# Drop temporary columns and keep formatted SSNs\n",
    "deduplicated_data = deduplicated_data.drop(\"first_name_lower\", \"middle_name_lower\", \"last_name_lower\") \\\n",
    "                                     .drop(\"ssn\").withColumnRenamed(\"formatted_ssn\", \"ssn\")\n",
    "```\n",
    "- **`drop(\"column_name\")`**: This method drops columns from the DataFrame. We drop the temporary lowercase columns (`first_name_lower`, `middle_name_lower`, `last_name_lower`) because we don’t need them anymore.\n",
    "  \n",
    "- **`withColumnRenamed(\"old_column\", \"new_column\")`**: This renames a column. Here, we rename the `formatted_ssn` column back to `ssn`, replacing the original, inconsistent `ssn` column.\n",
    "\n",
    "### **Block 7: Write the Final Data to a Parquet File**\n",
    "```python\n",
    "# Write to Parquet with 8 part files\n",
    "deduplicated_data.repartition(8).write.parquet(\"path_to_destination\", mode=\"overwrite\")\n",
    "```\n",
    "- **`repartition(8)`**: This repartitions the data into 8 parts, which ensures that when we write the data, we will end up with 8 Parquet files.\n",
    "  \n",
    "- **`write.parquet(\"path_to_destination\", mode=\"overwrite\")`**: This writes the DataFrame to the specified path in Parquet format. The `mode=\"overwrite\"` option means that if a file with the same name already exists at that location, it will be replaced.\n",
    "\n",
    "### Summary:\n",
    "1. We loaded the dataset and cleaned it up using UDFs to format names and SSNs.\n",
    "2. We removed duplicates based on case-insensitive name matching.\n",
    "3. We preserved the original formatting, fixed inconsistent SSNs, and wrote the cleaned data to a Parquet file with exactly 8 part files.\n",
    "\n",
    "This approach ensures we meet the requirements of deduplication and consistent data formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcee1e4f-cef7-4fad-bb4f-aec34635e37b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/data.gov/README.md</td><td>README.md</td><td>400</td><td>1459052893000</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/</td><td>farmers_markets_geographic_data/</td><td>0</td><td>1729513631556</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/irs_zip_code_data/</td><td>irs_zip_code_data/</td><td>0</td><td>1729513631556</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/data.gov/README.md",
         "README.md",
         400,
         1459052893000
        ],
        [
         "dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/",
         "farmers_markets_geographic_data/",
         0,
         1729513631556
        ],
        [
         "dbfs:/databricks-datasets/data.gov/irs_zip_code_data/",
         "irs_zip_code_data/",
         0,
         1729513631556
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/data.gov'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48c280e-32c3-4fa1-847a-be91f21db7e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bcefbb6-329b-45e9-8849-6b095d956056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Hints\n",
    "\n",
    "* Use the <a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">API docs</a>. Specifically, you might find \n",
    "  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\" target=\"_blank\">DataFrame</a> and\n",
    "  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\" target=\"_blank\">functions</a> to be helpful.\n",
    "* It's helpful to look at the file first, so you can check the format. `dbutils.fs.head()` (or just `%fs head`) is a big help here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80d630a-d95a-416d-a29d-8b87b9c5df74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: False"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "sourceFile = \"dbfs:/databricks-datasets/samples/people/people.json\"\n",
    "destFile = userhome + \"/people.parquet\"\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(destFile, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861ec51c-8a9e-4525-8dea-170873f2d676",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th></tr></thead><tbody><tr><td>40</td><td>Jane</td></tr><tr><td>30</td><td>Andy</td></tr><tr><td>50</td><td>Justin</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         40,
         "Jane"
        ],
        [
         30,
         "Andy"
        ],
        [
         50,
         "Justin"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sourceFile = \"dbfs:/databricks-datasets/samples/people/people.json\"\n",
    "peopleDF = spark.read.json(sourceFile)\n",
    "display(peopleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8325fbfb-7e83-46c6-81f5-65deae89d551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>{\"name\":\"Jane\"</th><th>\"age\":40}</th></tr></thead><tbody><tr><td>{\"name\":\"Andy\"</td><td>\"age\":30}</td></tr><tr><td>{\"name\":\"Justin\"</td><td>\"age\":50}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"name\":\"Andy\"",
         "\"age\":30}"
        ],
        [
         "{\"name\":\"Justin\"",
         "\"age\":50}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "{\"name\":\"Jane\"",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "\"age\":40}",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sourceDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(sourceFile)\n",
    "display(sourceDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8e6a4e-c7c6-4561-b0c5-325bf9ab5ece",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Validate Your Answer\n",
    "\n",
    "At the bare minimum, we can verify that you wrote the parquet file out to **destFile** and that you have the right number of records.\n",
    "\n",
    "Running the following cell to confirm your result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3131c096-6fae-4ef9-a8ff-ffeeeb05deea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# partFiles = len(list(filter(lambda f: f.path.endswith(\".parquet\"), dbutils.fs.ls(destFile))))\n",
    "\n",
    "# finalDF = spark.read.parquet(destFile)\n",
    "# finalCount = finalDF.count()\n",
    "\n",
    "# clearYourResults()\n",
    "# validateYourAnswer(\"01 Parquet File Exists\", 1276280174, partFiles)\n",
    "# validateYourAnswer(\"02 Expected 100000 Records\", 972882115, finalCount)\n",
    "# summarizeYourResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e108e6-4217-47aa-8424-bb61a87dcdbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 796006902875994,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Exercise User Defined Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
