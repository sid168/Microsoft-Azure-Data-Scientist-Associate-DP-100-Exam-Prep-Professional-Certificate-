{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38bac1e6-2ecc-415e-a588-47274d8edcc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Horovod with Petastorm\n",
    "\n",
    "[Petastorm](https://github.com/uber/petastorm) enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as TensorFlow, Pytorch, and PySpark and can be used from pure Python code.\n",
    "\n",
    "**Required Libraries**: \n",
    "* `petastorm==0.8.2` via PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfab54da-09ac-408f-aa43-0b2f1fa13b7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Imagine you and a bunch of friends are working together to build a super big LEGO castle. Each friend is in a different part of the room, and you all have your own pile of LEGO blocks. You want to build faster, so you each start building different parts of the castle at the same time, instead of waiting for one person to do it all.\n",
    "\n",
    "- **Petastorm** is the helper who gives each of you just the right LEGO blocks you need to build your part of the castle. Instead of having one giant pile that everyone fights over, Petastorm organizes it, making sure everyone has their own pile to build with.\n",
    "\n",
    "- **Horovod** is like the messenger who runs back and forth between all of you. It makes sure that everyone is building the castle in the same way, sharing any cool tricks or fixes along the way. If someone finds a better way to put pieces together, Horovod tells everyone else, so you all stay in sync.\n",
    "\n",
    "So, Petastorm gives each friend the pieces they need, and Horovod helps everyone work together smoothly, making sure the LEGO castle comes together perfectly and really fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82cebffd-c0c5-44da-a5a1-860e8452813d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cba74c8-0369-44f9-8a03-b9c4dca8ffe1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2bb448f-7a10-4c80-a4ef-275493a093d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be32f19-3267-4662-9d76-ad6d1c8e52bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "# split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
    "                                                        cal_housing.target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18541a51-e6d2-4bf8-b1dc-ded56d5a68e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "Let's concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4865c4d3-16d0-43ae-872d-c954552910f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.concat([pd.DataFrame(X_train, columns=cal_housing.feature_names), pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\n",
    "trainDF = spark.createDataFrame(data)\n",
    "display(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a5dad4e-ff21-461b-8a87-c01538fb3344",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Dense Vectors for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1844382e-0d84-44eb-95ba-b93566ea083a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\n",
    "display(vecTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9233c1e0-92a5-47f4-a3b1-01f0d6992022",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Array\n",
    "\n",
    "Petastorm requires an Array as input, not a Vector. Let's register a UDF in Scala and invoke it from Python for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8b08c84-c162-4726-a6d9-c97650b35913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "val toArray = udf { v: Vector => v.toArray }\n",
    "spark.udf.register(\"toArray\", toArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15699b79-34c5-4215-af06-7e231eff8adf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Imagine you have a special box of crayons (we'll call this a **vector**) where all the crayons are organized in a specific way—maybe in rows or slots. But you want them just laid out in a single line (an **array**) so you can see each color one after another without the slots.\n",
    "\n",
    "In this case:\n",
    "1. **Vector** is like your crayon box, where the crayons are organized in a specific structure.\n",
    "2. **Array** is like a row of crayons lined up one by one.\n",
    "\n",
    "#### What This Code Does\n",
    "\n",
    "1. **Define the Function** (`toArray`): This function (`toArray`) tells Spark how to take crayons out of the box (convert a vector to an array). It’s written in **Scala**, which is a language Spark understands well.\n",
    "   \n",
    "   ```scala\n",
    "   val toArray = udf { v: Vector => v.toArray }\n",
    "   ```\n",
    "\n",
    "   - **Scala UDF** (User-Defined Function): We create this function in Scala to take each vector (organized crayons) and flatten it out into a single line (array of crayons).\n",
    "\n",
    "2. **Register the Function** with Spark:\n",
    "   \n",
    "   ```scala\n",
    "   spark.udf.register(\"toArray\", toArray)\n",
    "   ```\n",
    "\n",
    "   - By registering `toArray`, we make it available for **Python code** in Spark. So now, Python can use this function to take any vector in a Spark DataFrame and turn it into an array.\n",
    "\n",
    "3. **Why This Matters for Petastorm**: \n",
    "   - Petastorm needs the crayon colors (data) laid out in a single row (array) instead of in a structured box (vector). So, we register `toArray` to help make this format change before Petastorm can use the data.\n",
    "\n",
    "In simpler terms, we’re creating a special “crayon layout” tool in Scala that Python code can use to change the shape of data, making it easy for Petastorm to work with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a70bf09c-9c34-4993-89d9-61351c088a0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save Data \n",
    "\n",
    "Let's write our DataFrame out as a parquet files to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37502c50-186f-434d-88ca-e42bff97b974",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = f\"{workingDir}/deep-learning/petastorm.parquet\"\n",
    "vecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b01d71-e622-409e-aaab-6feb2bebc41b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Remove committed/started metadata\n",
    "\n",
    "Petastorm + Horovod do not work if you leave the committed/started metadata files in our Parquet folder. We will need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ba460e-0c4e-4fdd-b438-971bbd154896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]\n",
    "\n",
    "display(dbutils.fs.ls(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a2efbd-bcb7-4bf2-b1d2-7bad30a93fc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34792ba-59fe-4c8b-a3ec-629419c2c013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "def build_model():\n",
    "  from tensorflow.keras import models, layers\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(20, input_dim=8, activation='relu'))\n",
    "  model.add(layers.Dense(20, activation='relu'))\n",
    "  model.add(layers.Dense(1, activation='linear'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58f9a627-6319-4c08-a228-01e4692d339d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Single Node\n",
    "\n",
    "Define shape of the input tensor and output tensor and fit the model (on the driver). We need to use Petastorm's [make_batch_reader](https://petastorm.readthedocs.io/en/latest/api.html#petastorm.reader.make_batch_reader) to create an instance of Reader for reading batches out of a non-Petastorm Parquet store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09c533b-74d2-4dd9-90b5-1c5bbcdfe91f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "abs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "\n",
    "with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None) as reader: \n",
    "  dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n",
    "  model = build_model()\n",
    "  optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='mse',\n",
    "                metrics=['mse'])\n",
    "  model.fit(dataset, steps_per_epoch=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66421107-b288-4a09-9af3-3f2ad9f4b268",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Horovod\n",
    "\n",
    "Let's do the same thing, but let's add in Horovod for distributed model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f468e8-d0fa-45c2-9763-fa680c2b4ac0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import horovod.tensorflow.keras as hvd\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count= hvd.size()) as reader:\n",
    "    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n",
    "    model = build_model()\n",
    "    from tensorflow.keras import optimizers\n",
    "    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    history = model.fit(dataset, steps_per_epoch=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7dfbf04-82cd-4634-8e06-553c309b38cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train on driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f3449e-ff6d-4d54-b00e-92ab9715cbec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4000258e-3110-43c7-8bcc-3c5efe06fa78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Better Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d660b9-81a8-4007-bf16-1a49aa9aadc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import horovod.tensorflow.keras as hvd\n",
    "\n",
    "\n",
    "dbutils.fs.rm(f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\", True)\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size()) as reader:\n",
    "    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n",
    "    model = build_model()\n",
    "    from tensorflow.keras import optimizers\n",
    "    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mse'])\n",
    "    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\"\n",
    "    callbacks = [\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    if hvd.rank() == 0:\n",
    "      callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n",
    "  \n",
    "    history = model.fit(dataset, steps_per_epoch=10, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5413b23f-12c3-44f5-91f9-32f3d2b5526d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import horovod.tensorflow.keras as hvd\n",
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17701fb1-c3dc-4f7e-aa4c-6ffead923de6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run on all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a697a54-5692-4729-bc51-1fdc83863801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=0)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "959bcd56-b04e-48ce-bc92-f011b7112532",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's go through each step -\n",
    "\n",
    "### 6. **Saving Data to Parquet Format**\n",
    "\n",
    "Imagine you just baked a batch of cookies, and you need to put them in little bags so your friends can each grab a bag. Here:\n",
    "- **Saving as Parquet**: We’re saving our data (like the cookies) in a special format called **Parquet**, which makes it easy for Petastorm to read.\n",
    "  \n",
    "   ```python\n",
    "   file_path = f\"{workingDir}/deep-learning/petastorm.parquet\"\n",
    "   vecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)\n",
    "   ```\n",
    "\n",
    "- **Repartitioning**: Instead of putting all cookies in one giant bag, we split them into 8 smaller bags. This way, when training starts, it’s faster for each computer to pick up a bag of cookies (data) and start working.\n",
    "\n",
    "### 7. **Removing Metadata Files**\n",
    "\n",
    "Some extra \"sticky notes\" or tags (metadata files) get left behind in our Parquet folder, which can confuse Petastorm and Horovod when they try to read data for training. We remove these extra tags so training runs smoothly.\n",
    "\n",
    "   ```python\n",
    "   [dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]\n",
    "   ```\n",
    "\n",
    "### 8. **Defining the Model**\n",
    "\n",
    "Now we’re building our \"brain\" model, or neural network, which will learn from the housing data and make predictions. Think of this as setting up a robot that will be taught to guess house prices.\n",
    "\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "   from tensorflow import keras\n",
    "   from tensorflow.keras import models, layers\n",
    "\n",
    "   def build_model():\n",
    "       model = models.Sequential()\n",
    "       model.add(layers.Dense(20, input_dim=8, activation='relu'))\n",
    "       model.add(layers.Dense(20, activation='relu'))\n",
    "       model.add(layers.Dense(1, activation='linear'))\n",
    "       return model\n",
    "   ```\n",
    "\n",
    "- **Sequential Model**: We create a model with two layers to process the data and an output layer to predict the house price.\n",
    "\n",
    "### 9. **Single Node Training**\n",
    "\n",
    "This is where we begin training the model. We take small batches of data from the Parquet file and give them to our model to learn step-by-step.\n",
    "\n",
    "   ```python\n",
    "   from petastorm import make_batch_reader\n",
    "   from petastorm.tf_utils import make_petastorm_dataset\n",
    "\n",
    "   abs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "   with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None) as reader:\n",
    "       dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n",
    "       model = build_model()\n",
    "       optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "       model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "       model.fit(dataset, steps_per_epoch=10, epochs=10)\n",
    "   ```\n",
    "\n",
    "- **Petastorm Reader**: It’s like having a teacher who hands out homework one page at a time so the model can practice on small pieces instead of the whole dataset at once.\n",
    "- **Model Training**: We tell the model to minimize mistakes (using the \"mse\" loss function) as it learns, making better predictions over time.\n",
    "\n",
    "### 10. **Distributed Training with Horovod**\n",
    "\n",
    "Now we want to speed up training by splitting the work among multiple computers. Horovod makes it possible for these computers to work together on the same task, so they each work on a portion of the data and share their progress.\n",
    "\n",
    "   ```python\n",
    "   import horovod.tensorflow.keras as hvd\n",
    "\n",
    "   def run_training_horovod():\n",
    "       hvd.init()\n",
    "       with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count= hvd.size()) as reader:\n",
    "           dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n",
    "           model = build_model()\n",
    "           optimizer = keras.optimizers.Adam(lr=0.001 * hvd.size())\n",
    "           optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "           model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "           history = model.fit(dataset, steps_per_epoch=10, epochs=10)\n",
    "   ```\n",
    "\n",
    "- **Horovod Initialization**: This step organizes each computer’s role, giving each computer a \"shard\" or part of the data, like assigning each friend a part of a puzzle to work on.\n",
    "- **Distributed Training**: Each computer works on its shard of data, and Horovod keeps them all in sync, so the model learns faster than if just one computer was working on the task.\n",
    "\n",
    "### 11. **Running Horovod on All Workers**\n",
    "\n",
    "Finally, we need to launch our training function across all computers in the cluster. The HorovodRunner helps us do this, sending the `run_training_horovod` function to all computers so they can work together.\n",
    "\n",
    "   ```python\n",
    "   from sparkdl import HorovodRunner\n",
    "   hr = HorovodRunner(np=-1)\n",
    "   hr.run(run_training_horovod)\n",
    "   ```\n",
    "\n",
    "- **Horovod Runner**: Think of it as the coordinator who tells everyone (all computers) to start working on their pieces of the data. The computers work in sync to complete the task quickly.\n",
    "\n",
    "And that’s how everything works together to train the model! Petastorm helps with reading the data, Horovod splits the task among many computers, and together they make training faster and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a528194c-acb7-439b-84cd-27648615779b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Horovod Petastorm",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
