{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0210b82-45a8-4a2a-a6b9-95e3dea3f0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading Data Lab\n",
    "* The goal of this lab is to put into practice some of what you have learned about reading data with Apache Spark.\n",
    "* The instructions are provided below along with empty cells for you to do your work.\n",
    "* At the bottom of this notebook are additional cells that will help verify that your work is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c034eb4-543c-43fe-9647-c3835bcd8449",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "0. Start with the file **dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv**, some random file you haven't seen yet.\n",
    "0. Read in the data and assign it to a `DataFrame` named **testDF**.\n",
    "0. Run the last cell to verify that the data was loaded correctly and to print its schema.\n",
    "0. The one untestable requirement is that you should be able to create the `DataFrame` and print its schema **without** executing a single job.\n",
    "\n",
    "**Note:** For the test to pass, the following columns should have the specified data types:\n",
    " * **prev_id**: integer\n",
    " * **curr_id**: integer\n",
    " * **n**: integer\n",
    " * **prev_title**: string\n",
    " * **curr_title**: string\n",
    " * **type**: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa0effa7-68d2-4298-a5fb-b5e9e22c6555",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e6bf135-2dea-4428-b8fc-aac6d2a32ed3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b73c36b-18c1-4242-ad85-e5e807ba4a9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Show Your Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c3cd9c-f9bd-4b8e-801d-ea80f95d856f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed/2015_2_clickstream.tsv</td><td>2015_2_clickstream.tsv</td><td>1322171548</td><td>1448260331000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed/2015_2_clickstream.tsv",
         "2015_2_clickstream.tsv",
         1322171548,
         1448260331000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed/2015_2_clickstream.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb318688-b1b3-43a7-b6c2-1dd2457a9e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "fileName = \"dbfs:/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed/2015_2_clickstream.tsv\"\n",
    "\n",
    "# testDF = (spark.read           # The DataFrameReader\n",
    "#    .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "#    .csv(fileName)               # Creates a DataFrame from CSV after reading in the file\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40574243-fdbe-4b82-88f9-ed60acf6e895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "  StructField(\"prev_id\", IntegerType(), False),\n",
    "  StructField(\"curr_id\", IntegerType(), False),\n",
    "  StructField(\"n\", IntegerType(), False),\n",
    "  StructField(\"prev_title\", StringType(), False),\n",
    "  StructField(\"curr_title\", StringType(), False),\n",
    "  StructField(\"type\", StringType(), False)\n",
    "  \n",
    "])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "748c7139-9fe5-44bb-a40d-174f5fc42fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- prev_id: integer (nullable = true)\n |-- curr_id: integer (nullable = true)\n |-- n: integer (nullable = true)\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "testDF = (spark.read                   # The DataFrameReader\n",
    "  .option('header', 'true')   # Ignore line #1 - it's a header\n",
    "  .option('sep', \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "  .schema(csvSchema)          # Use the specified schema\n",
    "  .csv(fileName)               # Creates a DataFrame from CSV after reading in the file\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "debe201f-14f6-4870-94af-f4f41613e9c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Verify Your Work\n",
    "Run the following cell to verify that your `DataFrame` was created properly.\n",
    "\n",
    "**Remember:** This should execute without triggering a single job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c7ee5a-cb7a-4859-a53e-8eaa033934cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- prev_id: long (nullable = true)\n |-- curr_id: long (nullable = true)\n |-- n: long (nullable = true)\n |-- prev_title: string (nullable = true)\n |-- curr_title: string (nullable = true)\n |-- type: string (nullable = true)\n\nCongratulations, all tests passed... that is if no jobs were triggered :-)\n\n"
     ]
    }
   ],
   "source": [
    "testDF.printSchema()\n",
    "\n",
    "columns = testDF.dtypes\n",
    "assert len(columns) == 6, \"Expected 6 columns but found \" + str(len(columns))\n",
    "\n",
    "assert columns[0][0] == \"prev_id\",    \"Expected column 0 to be \\\"prev_id\\\" but found \\\"\" + columns[0][0] + \"\\\".\"\n",
    "assert columns[0][1] == \"bigint\",     \"Expected column 0 to be of type \\\"bigint\\\" but found \\\"\" + columns[0][1] + \"\\\".\"\n",
    "\n",
    "assert columns[1][0] == \"curr_id\",    \"Expected column 1 to be \\\"curr_id\\\" but found \\\"\" + columns[1][0] + \"\\\".\"\n",
    "assert columns[1][1] == \"bigint\",     \"Expected column 1 to be of type \\\"bigint\\\" but found \\\"\" + columns[1][1] + \"\\\".\"\n",
    "\n",
    "assert columns[2][0] == \"n\",          \"Expected column 2 to be \\\"n\\\" but found \\\"\" + columns[2][0] + \"\\\".\"\n",
    "assert columns[2][1] == \"bigint\",     \"Expected column 2 to be of type \\\"bigint\\\" but found \\\"\" + columns[2][1] + \"\\\".\"\n",
    "\n",
    "assert columns[3][0] == \"prev_title\", \"Expected column 3 to be \\\"prev_title\\\" but found \\\"\" + columns[3][0] + \"\\\".\"\n",
    "assert columns[3][1] == \"string\",     \"Expected column 3 to be of type \\\"string\\\" but found \\\"\" + columns[3][1] + \"\\\".\"\n",
    "\n",
    "assert columns[4][0] == \"curr_title\", \"Expected column 4 to be \\\"curr_title\\\" but found \\\"\" + columns[4][0] + \"\\\".\"\n",
    "assert columns[4][1] == \"string\",     \"Expected column 4 to be of type \\\"string\\\" but found \\\"\" + columns[4][1] + \"\\\".\"\n",
    "\n",
    "assert columns[5][0] == \"type\",       \"Expected column 5 to be \\\"type\\\" but found \\\"\" + columns[5][0] + \"\\\".\"\n",
    "assert columns[5][1] == \"string\",     \"Expected column 5 to be of type \\\"string\\\" but found \\\"\" + columns[5][1] + \"\\\".\"\n",
    "\n",
    "print(\"Congratulations, all tests passed... that is if no jobs were triggered :-)\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2966569289733659,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "6.Reading Data - Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
