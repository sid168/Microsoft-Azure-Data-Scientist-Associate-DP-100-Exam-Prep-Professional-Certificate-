{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45725cff-fff6-410d-9c85-0f4a137a5587",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exercise: Finish Featurizing the Dataset\n",
    "\n",
    "One common way of handling categorical data is to divide it into bins, a process technically known as discretizing.  For instance, the dataset contains a number of rating scores that can be translated into a value of `1` if they are a highly rated host or `0` if not.\n",
    "\n",
    "Finish featurizing the dataset by binning the review scores rating into high versus low rated hosts.  Also filter the extreme values and clean the column `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afe88cad-d043-402d-96e6-eb91eb25a573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8774ffb9-1d70-4943-8ed3-17e75a3ad523",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook walks through steps to complete a data preprocessing pipeline for an Airbnb dataset. \n",
    "\n",
    "### Step-by-Step Walkthrough\n",
    "\n",
    "#### Step 1: Setting Up the Environment\n",
    "The initial cell (`%run \"./Includes/Classroom-Setup\"`) runs a setup script that loads necessary libraries and mounts the Databricks dataset directory.\n",
    "\n",
    "#### Step 2: Load and Prepare Data\n",
    "In this step, the Airbnb dataset is loaded, and preliminary transformations are defined:\n",
    "\n",
    "1. **Data Loading**: The dataset is loaded from a `.parquet` file with:\n",
    "   ```python\n",
    "   airbnbDF = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/sf-listings-correct-types.parquet\")\n",
    "   ```\n",
    "2. **Encoding Categorical Data**:\n",
    "   - `StringIndexer` is used to convert the `room_type` column into numerical indices (like `0` for \"Private room\").\n",
    "   - `OneHotEncoder` then transforms these indices into a one-hot encoded format.\n",
    "   \n",
    "3. **Handling Missing Data**:\n",
    "   - An `Imputer` is used to fill missing values in numeric columns (`host_total_listings_count`, `bathrooms`, etc.) with the median values.\n",
    "\n",
    "4. **Pipeline Creation**:\n",
    "   - A `Pipeline` is created to chain these transformations (indexing, encoding, and imputing) into a single step.\n",
    "   - `pipelineModel.fit()` fits the pipeline to the data, and `transform()` is applied to generate `transformedDF`, which contains all the transformations.\n",
    "   \n",
    "   ```python\n",
    "   pipelineModel = pipeline.fit(airbnbDF)\n",
    "   transformedDF = pipelineModel.transform(airbnbDF)\n",
    "   ```\n",
    "\n",
    "#### Step 3: Binarizing Review Scores\n",
    "To classify listings into high- and low-rated hosts:\n",
    "\n",
    "1. **Binarizer Setup**:\n",
    "   - `Binarizer` is used to convert `review_scores_rating` values into binary classifications.\n",
    "   - A threshold of `4.0` is set, so ratings â‰¥ 4 are classified as 1 (high rating), and others as 0.\n",
    "   \n",
    "   ```python\n",
    "   binarizer = Binarizer(threshold=4.0, inputCol=\"review_scores_rating\", outputCol=\"high_rating\")\n",
    "   transformedBinnedDF = binarizer.transform(transformedDF)\n",
    "   ```\n",
    "\n",
    "2. **Validation**:\n",
    "   - The `dbTest()` function checks if the transformations are correctly applied by validating column types and values.\n",
    "\n",
    "#### Step 4: Cleaning the `price` Column\n",
    "To clean and convert the `price` column:\n",
    "\n",
    "1. **Removing Symbols**:\n",
    "   - Using `regexp_replace`, the code removes `$` and `,` symbols, converting the price to `decimal(10,2)` type.\n",
    "   \n",
    "   ```python\n",
    "   transformedBinnedRegexDF = transformedBinnedDF.withColumn(\"price_raw\", col(\"price\")).withColumn(\"price\", regexp_replace(col(\"price\"), \"[$,]\", \"\").cast(\"decimal(10,2)\"))\n",
    "   ```\n",
    "\n",
    "2. **Validation**:\n",
    "   - Another `dbTest()` validates that the column is converted correctly and includes a backup `price_raw` column.\n",
    "\n",
    "#### Step 5: Filtering Extreme Values\n",
    "To address extreme values in the dataset:\n",
    "\n",
    "1. **Removing Outliers**:\n",
    "   - This step filters out rows where `price` is less than or equal to $0 or where `minimum_nights` is 365 or higher.\n",
    "   \n",
    "   ```python\n",
    "   transformedBinnedRegexFilteredDF = transformedBinnedRegexDF.filter((col(\"price_raw\").isNotNull()) & (col(\"price\") > 0) & (col(\"minimum_nights\") < 365))\n",
    "   ```\n",
    "\n",
    "2. **Validation**:\n",
    "   - The final `dbTest()` checks that the correct number of rows remains after filtering.\n",
    "\n",
    "This completes the data preprocessing, preparing the dataset for machine learning tasks by ensuring that the data is clean, standardized, and properly formatted for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ed0e04a-7b68-4e17-92d2-fb1f2287eb0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a7abfb-056a-4f52-9da1-833a722fe2ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Restore the Dataset from the Featurization module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17d7083b-d623-45ab-b284-a419fd266a25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "airbnbDF = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/sf-listings-correct-types.parquet\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\")\n",
    "encoder = OneHotEncoder(inputCols=[\"room_type_index\"], outputCols=[\"encoded_room_type\"])\n",
    "imputeCols = [\n",
    "  \"host_total_listings_count\",\n",
    "  \"bathrooms\",\n",
    "  \"beds\", \n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\"\n",
    "]\n",
    "imputer = Imputer(strategy=\"median\", inputCols=imputeCols, outputCols=imputeCols)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "  indexer, \n",
    "  encoder, \n",
    "  imputer\n",
    "])\n",
    "\n",
    "pipelineModel = pipeline.fit(airbnbDF)\n",
    "transformedDF = pipelineModel.transform(airbnbDF)\n",
    "\n",
    "display(transformedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e3ad72-6d7d-4d8c-a590-f5d996bc66b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1: Binning `review_scores_rating`\n",
    "\n",
    "Divide the hosts by whether their `review_scores_rating` is above 97.  Do this using the transformer `Binarizer` with the output column `high_rating`.  This should create the objects `binarizer` and the transformed DataFrame `transformedBinnedDF`.\n",
    "\n",
    "<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Note that `Binarizer` is a transformer, so it does not have a `.fit()` method<br>\n",
    "<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=binarizer#pyspark.ml.feature.Binarizer\" target=\"_blank\">Binarizer Docs</a> for more details.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4027f334-0982-4e40-a40c-11ad80bd47ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=4.0, inputCol=\"review_scores_rating\", outputCol=\"high_rating\")\n",
    "transformedBinnedDF = binarizer.transform(transformedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2bd3d1-0a14-4c0f-a5d4-9b1d2af3c331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "dbTest(\"ML1-P-05-01-01\", True, type(binarizer) == type(Binarizer()))\n",
    "dbTest(\"ML1-P-05-01-02\", True, binarizer.getInputCol() == 'review_scores_rating')\n",
    "dbTest(\"ML1-P-05-01-03\", True, binarizer.getOutputCol() == 'high_rating')\n",
    "dbTest(\"ML1-P-05-01-04\", True, \"high_rating\" in transformedBinnedDF.columns)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4223dac6-2c5d-4056-b5ee-869aa10ab0ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2: Regular Expressions on Strings\n",
    "\n",
    "Clean the column `price` by creating two new columns:<br><br>\n",
    "\n",
    "1. `price`: a new column that contains a cleaned version of price.  This can be done using the regular expression replacement of `\"[\\$,]\"` with `\"\"`.  Cast the column as a decimal.\n",
    "2. `raw_price`: the collumn `price` in its current form\n",
    "\n",
    "<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=regexp_replace#pyspark.sql.functions.regexp_replace\" target=\"_blank\">`regex_replace` Docs</a> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813d3b8a-587a-469b-a83c-b11691e07d9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "transformedBinnedRegexDF = transformedBinnedDF.withColumn(\"price_raw\", col(\"price\")).withColumn(\"price\", regexp_replace(col(\"price\"), \"[$,]\", \"\").cast(\"decimal(10,2)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ccc9f2-e3c6-41da-a0e3-6882fd4326d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "dbTest(\"ML1-P-05-02-01\", True, type(transformedBinnedRegexDF.schema[\"price\"].dataType) == type(DecimalType()))\n",
    "dbTest(\"ML1-P-05-02-02\", True, \"price_raw\" in transformedBinnedRegexDF.columns)\n",
    "dbTest(\"ML1-P-05-02-03\", True, \"price\" in transformedBinnedRegexDF.columns)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec2d39a4-3464-413c-9420-36a95340b938",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3: Filter Extremes\n",
    "\n",
    "The dataset contains extreme values, including negative prices and minimum stays of over one year.  Filter out all prices of $0 or less and all `minimum_nights` of 365 or higher.  Save the results to `transformedBinnedRegexFilteredDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad76cb59-638a-495c-a798-6c2250d02765",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "transformedBinnedRegexFilteredDF = transformedBinnedRegexDF.filter((col(\"price_raw\").isNotNull()) & (col(\"price\") > 0) & (col(\"minimum_nights\") < 365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "700e4073-4f14-4914-ac41-a9d49cb62f76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TEST - Run this cell to test your solution\n",
    "dbTest(\"ML1-P-05-03-01\", 4789, transformedBinnedRegexFilteredDF.count())\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. Exercise Featurization",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
