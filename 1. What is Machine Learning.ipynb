{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dedb953a-b63d-4630-8fc1-8fe9d74505f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "Machine learning discovers patterns within data without being explicitly programmed.  This lesson introduces machine learning, explores the main topics in the field, and builds an end-to-end pipeline in Spark.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n",
    "* Define machine learning\n",
    "* Differentiate supervised and unsupervised tasks\n",
    "* Identify regression and classification tasks\n",
    "* Train a model, interpret the results, and create predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "354914cd-721e-4fab-b0d7-c5bdddbf33b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<iframe  \n",
    "src=\"//fast.wistia.net/embed/iframe/h8cqzugdrf?videoFoam=true\"\n",
    "style=\"border:1px solid #1cb1c2;\"\n",
    "allowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\n",
    "name=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\n",
    "oallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n",
    "<div>\n",
    "<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/h8cqzugdrf?seo=false\">\n",
    "  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af8152b-aff6-4311-95aa-23e4d8be1b74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Learning from Data\n",
    "\n",
    "Machine learning refers to a diverse set of tools for understanding data.  More technically, **machine learning is the process of _learning from data_ without being _explicitly programmed_**.  Let's unpack what that means.\n",
    "\n",
    "Take a dataset of Boston home values in the 1970's for example.  The dataset consists of the value of homes as well as the number of rooms, crime per capita, and percent of the population considered lower class.  Home value is the _output variable_, also known as the _label_.  The other variables are known as _input variables_ or _features_.\n",
    "\n",
    "A machine learning model would _learn_ the relationship between housing price and the various features without being explicitly programmed.  There are many possible functions that would map input features to the output variable.  In more technical terms, our model would learn a function `f()` that maps the relationship between input features and the output variable.\n",
    "\n",
    "The following image shows the relation between our features and house value.  A good model `f()` would learn from the data that the number of rooms in a home is positively correlated to the house value while crime and percent of the neighborhood that is lower class is negatively correlated.  \n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/boston-housing.png\" style=\"height: 400px; margin: 20px\"/></div>\n",
    "\n",
    "The lines above represent the best fit for the data where our model's best guess for a house price given a feature value on the X axis is the corresponding point on the line.\n",
    "\n",
    "**Machine learning is the set of approaches for estimating this function `f()` that maps features to an output.**  The inputs to this function can range from stock prices and customer information to images and DNA sequences.  Many of the same statistical techniques apply regardless of the domain.  This makes machine learning a generalizable skill set that drives decision-making in modern businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ea26f36-b0fd-46ef-b7a4-52e90c2eb047",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\"\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster. Click <b>Detached</b> in the upper left hand corner and then select your preferred cluster.\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/attach-to-cluster.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d6592b9-a5f4-4ad4-920e-fb9bc589d709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Initialized classroom variables & functions..."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Initialized classroom variables & functions...",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Datasets are already mounted to <b>/mnt/training</b> from <b>wasbs://training@dbtrainwesteurope.blob.core.windows.net/</b>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5459be05-69ea-43d6-82eb-f0c77b780ed6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Created user-specific database"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Created user-specific database",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "Using the database <b style=\"color:green\">saifahmed_k_outlook_com_db</b>.",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "All done!"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "All done!",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd248fc0-a9a3-4c91-882f-fbd5bdffdd1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Supervised vs Unsupervised Learning\n",
    "\n",
    "Machine learning problems are roughly categorized into two main types:<br><br>\n",
    "\n",
    "* **Supervised learning** looks to predict the value of some outcome based on one or more input measures\n",
    "  - Our example of the Boston Housing Dataset is an example of supervised learning\n",
    "  - In this case, the output is the price of a home and the input is features such as number of rooms\n",
    "* **Unsupervised learning** describes associations and patterns in data without a known outcome\n",
    "  - An example of this would be clustering customer data to find the naturally occurring customer segments\n",
    "  - In this case, no known output is used as an input.  Instead, the goal is to discover how the data are organized into natural segments or clusters\n",
    "\n",
    "This course will cover supervised learning, which is the vast majority of machine learning use cases in industry.  Later courses will look at unsupervised approaches.\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/regression.png\" style=\"height: 400px; margin: 20px\"/><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/clustering.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26477966-9c3c-42b7-a074-46370447f922",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the Boston dataset, which contains median house values in 1000's (`medv`) for a variety of different features.  Since this dataset is \"supervised\" my the median value, this is a supervised machine learning use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe25c9c-4b04-484b-9bb7-d612c67cde38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/mnt/training/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024f758f-8ae8-47ea-92cd-6dc9d9658155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bostonDF = (spark.read\n",
    "  .option(\"HEADER\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(\"/mnt/training/bostonhousing/bostonhousing/bostonhousing.csv\")\n",
    ")\n",
    "\n",
    "display(bostonDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce526dd-07b7-4496-95d5-5aeb60063590",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark differs from many other machine learning frameworks in that we train our model on a single column that contains a vector of all of our features.  Prepare the data by creating one column named `features` that has the average number of rooms, crime rate, and poverty percentage.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler\" target=\"_blank\">`VectorAssembler` documentation for more details.</a>\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> `VectorAssembler` is a tranformer, which implements a `.transform()` method.  Estimators, by contrast, need to learn from the data using a `.fit()` method before they can transform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d122a1d-4af4-4005-a8dd-160cf5ee6516",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureCols = [\"rm\", \"crim\", \"lstat\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "\n",
    "bostonFeaturizedDF = assembler.transform(bostonDF)\n",
    "\n",
    "display(bostonFeaturizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3571a47e-fc0a-457d-8f40-3f8afbaf4c83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureCols = [\"rm\", \"crim\", \"lstat\"]\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")\n",
    "```\n",
    "### Why Do We Need to Put Them in One Vector?\n",
    "\n",
    "Most machine learning algorithms in Spark (and other frameworks) expect the features to be in the form of a **single vector**. A vector is just a list of numbers, and it's easier for models to process this than handling many separate columns. By combining these columns into one vector, the algorithm can efficiently calculate predictions.\n",
    "\n",
    "Imagine you are predicting house prices, and you have three features:\n",
    "- `rm`: Average number of rooms.\n",
    "- `crim`: Crime rate.\n",
    "- `lstat`: Percentage of lower-income households.\n",
    "\n",
    "For example, let’s say you have a row in the dataset with the following values:\n",
    "- `rm = 6.0` (6 rooms)\n",
    "- `crim = 0.1` (low crime rate)\n",
    "- `lstat = 12.0` (12% lower-income population)\n",
    "\n",
    "Instead of keeping these values in separate columns, we combine them into a **feature vector** like this:\n",
    "```\n",
    "features = [6.0, 0.1, 12.0]\n",
    "```\n",
    "\n",
    "#### Why Combine Them?\n",
    "- **Efficiency**: Machine learning algorithms need to process many features at once. It's faster and more efficient to work with a single vector (list of numbers) than with multiple columns.\n",
    "  \n",
    "- **Mathematical Operations**: The model often needs to perform operations like matrix multiplication or dot products on the features. These operations are easier when all the features are stored in one vector.\n",
    "  \n",
    "- **Example of How the Model Uses the Vector**: \n",
    "   Suppose we are training a **linear regression model**. The model might learn an equation like this to predict house prices:\n",
    "   ```\n",
    "   predicted_price = (5.2 * rm) - (0.1 * crim) - (0.6 * lstat) - 2.6\n",
    "   ```\n",
    "   To make this prediction for the above row, the model will compute:\n",
    "   ```\n",
    "   predicted_price = (5.2 * 6.0) - (0.1 * 0.1) - (0.6 * 12.0) - 2.6\n",
    "                   = 31.2 - 0.01 - 7.2 - 2.6\n",
    "                   = 21.39\n",
    "   ```\n",
    "\n",
    "In this case, having all the features in a vector `[6.0, 0.1, 12.0]` makes it easy for the model to apply these calculations.\n",
    "\n",
    "- We need to combine them into one vector because machine learning algorithms work more efficiently with data in this format, allowing them to process many features together and apply mathematical operations for training and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24baa040-8ec3-4a52-ae94-d4a72371c3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The line of code:\n",
    "\n",
    "```python\n",
    "bostonFeaturizedDF = assembler.transform(bostonDF)\n",
    "```\n",
    "\n",
    "is performing a **data transformation** step using the `VectorAssembler` object (`assembler`). Here's what it does in detail:\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Input: `bostonDF` (Original DataFrame)**:\n",
    "   - `bostonDF` is the original Spark DataFrame containing the Boston Housing dataset. It has multiple columns, including features like `rm` (average number of rooms), `crim` (crime rate), and `lstat` (percentage of lower-income population), as well as the target variable (`medv`), which is the median house value.\n",
    "\n",
    "2. **`assembler.transform()`**:\n",
    "   - The `assembler` is an instance of `VectorAssembler`, which was created earlier with a list of input columns (`[\"rm\", \"crim\", \"lstat\"]`) and an output column (`\"features\"`).\n",
    "   - The `.transform(bostonDF)` method is applied to the original DataFrame (`bostonDF`). It combines the specified input columns (`rm`, `crim`, `lstat`) into a single **feature vector** column, called `\"features\"`.\n",
    "\n",
    "3. **Output: `bostonFeaturizedDF` (Transformed DataFrame)**:\n",
    "   - After applying the transformation, a new DataFrame (`bostonFeaturizedDF`) is created. This DataFrame contains all the original columns, plus a new column called `features`, which holds the combined feature vectors for each row.\n",
    "   \n",
    "   - For example, for a row with:\n",
    "     - `rm = 6.0` (6 rooms),\n",
    "     - `crim = 0.1` (crime rate),\n",
    "     - `lstat = 12.0` (12% lower-income population),\n",
    "     \n",
    "     the new `features` column would contain:\n",
    "     ```\n",
    "     features = [6.0, 0.1, 12.0]\n",
    "     ```\n",
    "\n",
    "### Why It's Done\n",
    "\n",
    "- **Combining Features for Machine Learning**: Machine learning algorithms in Spark typically expect input features to be in a single column (a vector of numbers). By combining multiple feature columns into one vector column (`features`), the model can use this as input for training.\n",
    "- **Simplifies Data Processing**: Instead of passing multiple columns separately, it's easier and more efficient to pass one column that contains all the features.\n",
    "\n",
    "### Example Output Structure\n",
    "If `bostonDF` originally looked like this:\n",
    "\n",
    "| rm  | crim  | lstat | medv  |\n",
    "|-----|-------|-------|-------|\n",
    "| 6.0 | 0.1   | 12.0  | 21.2  |\n",
    "| 7.0 | 0.2   | 15.0  | 25.3  |\n",
    "\n",
    "After transformation with `VectorAssembler`, `bostonFeaturizedDF` will look like this:\n",
    "\n",
    "| rm  | crim  | lstat | medv  | features       |\n",
    "|-----|-------|-------|-------|----------------|\n",
    "| 6.0 | 0.1   | 12.0  | 21.2  | [6.0, 0.1, 12.0] |\n",
    "| 7.0 | 0.2   | 15.0  | 25.3  | [7.0, 0.2, 15.0] |\n",
    "\n",
    "Now the `features` column contains a vector of the selected features (`rm`, `crim`, `lstat`), which can be fed into a machine learning model for training.\n",
    "\n",
    "### Summary:\n",
    "The line `bostonFeaturizedDF = assembler.transform(bostonDF)` takes the original DataFrame (`bostonDF`) and combines the selected feature columns (`rm`, `crim`, `lstat`) into a single `features` column, creating a new DataFrame (`bostonFeaturizedDF`) that is ready for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b445de4-5be3-480a-b6b3-2ad422a78d01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now have the input features and the output, median value, which appears in the data as `medv`.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://spark.apache.org/docs/latest/mllib-data-types.html\" target=\"_blank\">the MLlib documentation for more details</a> on the sparse vector notation seen in the `features` column, which will be covered in more detail in the Featurization lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3cf9a4a-4a56-48f2-8186-58ca232c9605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Regression vs Classification\n",
    "\n",
    "Variables can either be quantitative or qualitative:<br><br>\n",
    "\n",
    "* **Quantitative** values are numeric and generally unbounded, taking any positive or negative value\n",
    "* **Qualitative** values take on a set number of classes or categories\n",
    "\n",
    "| Variable type    | Also known as         | Examples                                                          |\n",
    "|:-----------------|:----------------------|:------------------------------------------------------------------|\n",
    "| quantitative     | continuous, numerical | age, salary, temperature                                          |\n",
    "| qualitative      | categorical, discrete | gender, whether or a not a patient has cancer, state of residence |\n",
    "\n",
    "Machine learning models operate on numbers so a qualitative variable like gender, for instance, would need to be encoded as `0` for male or `1` for female.  In this case, female isn't \"one more\" than male, so this variable is handled differently compared to a quantitative variable.\n",
    "\n",
    "Generally speaking, **a supervised model learning a quantitative variable is called regression and a model learning a qualitative variable is called classification.**\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/classification_v_regression.jpg\" style=\"height: 400px; margin: 20px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81259ac7-d6b4-4329-9cf3-7aa6e94b1ceb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import linear regression, one way of modeling continuous variables.  Set the output to be the `medv` variable and the input to be the `features` made above.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.regression.LinearRegression\" target=\"_blank\">LinearRegression documentation for more details.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed5964e8-a501-4f68-a84d-dc41108a57b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"medv\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53ca949-ac59-4ecc-8095-772e38cf0397",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fit the model to the data using the `.fit()` method.  This returns a trained model that has learned from our data and will be stored in `lrModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b161342a-aebd-4169-b2c2-69651235034b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lrModel = lr.fit(bostonFeaturizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bea55d6-6df6-4385-88be-80f57e2f7568",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take a look at what the model learned from our data.  In the case of linear regression, this comes in the form of an equation that describes the relationship the model has just learned.  This will be covered in more detail in later lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d169b868-fac4-4e8c-af14-c1e20b87ee5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Coefficients: {0:.1f}, {1:.1f}, {2:.1f}\".format(*lrModel.coefficients))\n",
    "print(\"Intercept: {0:.1f}\".format(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5685c4e5-7278-43de-a06a-01de4ceee6be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The code:\n",
    "\n",
    "```python\n",
    "print(\"Coefficients: {0:.1f}, {1:.1f}, {2:.1f}\".format(*lrModel.coefficients))\n",
    "print(\"Intercept: {0:.1f}\".format(lrModel.intercept))\n",
    "```\n",
    "\n",
    "is displaying the **coefficients** and **intercept** of a **linear regression model** that has been trained. Here's what each part of this code does:\n",
    "\n",
    "### 1. **Displaying the Coefficients**\n",
    "\n",
    "```python\n",
    "print(\"Coefficients: {0:.1f}, {1:.1f}, {2:.1f}\".format(*lrModel.coefficients))\n",
    "```\n",
    "\n",
    "- **What are coefficients?**\n",
    "  - Coefficients represent the weight or influence each feature has on the prediction made by the linear regression model. Each coefficient corresponds to one of the input features. They show how much the target variable (e.g., house price) changes when the corresponding feature changes, assuming the other features remain constant.\n",
    "\n",
    "- **Breakdown of the code:**\n",
    "  - `lrModel.coefficients`: This retrieves the coefficients from the trained linear regression model (`lrModel`). In this case, `lrModel.coefficients` would be a vector (list of numbers) containing three values because the model is using three features (`rm`, `crim`, `lstat`).\n",
    "  - `*lrModel.coefficients`: The `*` unpacks the list of coefficients into separate arguments for the `format()` function.\n",
    "  - `\"{0:.1f}, {1:.1f}, {2:.1f}\".format(...)`: The `.format()` method inserts the coefficients into the string. The `{:.1f}` specifies that each number will be formatted to one decimal place.\n",
    "  \n",
    "- **Example**: If the coefficients are `[5.2, -0.1, -0.6]`, the output will be:\n",
    "  ```\n",
    "  Coefficients: 5.2, -0.1, -0.6\n",
    "  ```\n",
    "  This means:\n",
    "  - For every additional room (`rm`), the predicted house price increases by 5.2 (assuming the other features are held constant).\n",
    "  - For every 1-point increase in the crime rate (`crim`), the predicted house price decreases by 0.1.\n",
    "  - For every 1% increase in the lower-income population (`lstat`), the predicted house price decreases by 0.6.\n",
    "\n",
    "### 2. **Displaying the Intercept**\n",
    "\n",
    "```python\n",
    "print(\"Intercept: {0:.1f}\".format(lrModel.intercept))\n",
    "```\n",
    "\n",
    "- **What is the intercept?**\n",
    "  - The intercept is the baseline value of the target variable (in this case, the house price) when all the features are equal to zero. It’s the starting point of the prediction when none of the input features have any effect (i.e., they are zero).\n",
    "\n",
    "- **Breakdown of the code:**\n",
    "  - `lrModel.intercept`: This retrieves the intercept value from the trained model.\n",
    "  - `\"{0:.1f}\".format(...)`: This formats the intercept to one decimal place and inserts it into the string.\n",
    "\n",
    "- **Example**: If the intercept is `-2.6`, the output will be:\n",
    "  ```\n",
    "  Intercept: -2.6\n",
    "  ```\n",
    "  This means that if the values for `rm`, `crim`, and `lstat` were all zero, the predicted house price would start at -2.6 (which is a hypothetical value since these features are unlikely to be exactly zero in real life).\n",
    "\n",
    "### Why Are Coefficients and Intercept Important?\n",
    "\n",
    "In a **linear regression** model, the equation for making predictions is:\n",
    "```\n",
    "predicted_value = (coefficient_1 * feature_1) + (coefficient_2 * feature_2) + ... + intercept\n",
    "```\n",
    "\n",
    "For this example:\n",
    "```\n",
    "predicted_price = (5.2 * rm) + (-0.1 * crim) + (-0.6 * lstat) - 2.6\n",
    "```\n",
    "\n",
    "These coefficients and the intercept tell you how the model is calculating its predictions based on the input features.\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "If your model has learned the following:\n",
    "- Coefficients: `5.2`, `-0.1`, `-0.6`\n",
    "- Intercept: `-2.6`\n",
    "\n",
    "The output would be:\n",
    "```\n",
    "Coefficients: 5.2, -0.1, -0.6\n",
    "Intercept: -2.6\n",
    "```\n",
    "\n",
    "This tells you that:\n",
    "- The house price increases by 5.2 for each additional room.\n",
    "- The house price decreases by 0.1 for each increase in the crime rate.\n",
    "- The house price decreases by 0.6 for each 1% increase in the lower-income population.\n",
    "- The starting point (intercept) is -2.6.\n",
    "\n",
    "### Summary:\n",
    "- The first `print()` line displays the coefficients (weights) for each feature (`rm`, `crim`, and `lstat`), showing how they influence the predicted house price.\n",
    "- The second `print()` line displays the intercept, which is the base value when all features are zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63426bf5-33e7-4978-b225-41f863c09893",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The coefficients of our model are a multiplier against our original features and the intercept is a constant.  To interpret our model, use the following equation:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`predicted home value = (5.2 x number of rooms) - (.1 x crime rate) - (.6 x % lower class) - 2.6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93feaa79-b139-44ff-81e7-aa061e580b59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prediction\n",
    "\n",
    "So far, we've accomplished the following:<br><br>\n",
    "\n",
    "* Prepared data for a machine learning model\n",
    "* Trained a linear regression model\n",
    "* Interpreted what it learned about our data\n",
    "\n",
    "Now, let's see how it predicts on data it has already seen as well as new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95954e0a-c64a-434f-a5bb-648a7f194274",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create a subset of the first 10 rows of the `features` and `medv` target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132d3375-1bcc-46ae-a25e-e6bdd5f15849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subsetDF = (bostonFeaturizedDF\n",
    "  .limit(10)\n",
    "  .select(\"features\", \"medv\")\n",
    ")\n",
    "\n",
    "display(subsetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186c1921-888a-49db-81ac-7faaee4b1d17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the `transform` method on the trained model to see its prediction.\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Now that `lrModel` is a trained estimator, we can transform data using its `.transform()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b12ed5ff-32f7-4e3c-82da-5b201d3c457f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictionDF = lrModel.transform(subsetDF)\n",
    "\n",
    "display(predictionDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423ba6e4-a537-4177-8cda-fc4e580a2d9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How did the model do?  While there's a difference between the prediction and the true value (also known as the model error), generally the model seems to have learned something about our data.\n",
    "\n",
    "Now predict off of a hypothetical data point of a 6 bedroom home with a 3.6 crime rate and 12 % average lower class.  According to our formula, the model should predict about 21:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`predicted home value = (5.2 x number of rooms) - (.1 x crime rate) - (.6 x % lower class) - 2.6`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`predicted home value = (5.2 x 6) - (.1 x 3.6) - (.6 x 12) - 2.6`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`predicted home value = 31.2 - .4 - 7.2 - 2.6`<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`predicted home value = 21`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46da7327-faea-448f-ba00-a78f6dd71f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.dense([6., 3.6, 12.]), )]              # Creates our hypothetical data point\n",
    "predictDF = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "display(lrModel.transform(predictDF))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3866166378674542,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1. What is Machine Learning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
