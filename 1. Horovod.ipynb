{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775a56f4-2239-468c-aa21-881906036d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Horovod\n",
    "\n",
    "HorovodRunner is a general API to run distributed DL workloads on Databricks using Uber’s [Horovod](https://github.com/uber/horovod) framework. By integrating Horovod with Spark’s barrier mode, Databricks is able to provide higher stability for long-running deep learning training jobs on Spark. HorovodRunner takes a Python method that contains DL training code with Horovod hooks. This method gets pickled on the driver and sent to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled program back, deserializes it, and runs it.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://files.training.databricks.com/images/horovod-runner.png)\n",
    "\n",
    "For additional resources, see:\n",
    "* [Horovod Runner Docs](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/distributed-training/horovod-runner)\n",
    "* [Horovod Runner webinar](https://vimeo.com/316872704/e79235f62c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66cda9af-f842-4786-aa94-ed9b232a1ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c47df75-89f6-4ae5-b729-0af95e0f98ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The execution of this command did not finish successfully",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3cb5903-266e-4e46-aa4a-775fa08fb437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3fae6f-a139-4968-b43d-63fd4bcbe4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2004014418861572>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_random_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m42\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# For reproducibility\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-2004014418861572>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_random_seed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m42\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# For reproducibility\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'tensorflow'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42) # For reproducibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_model():\n",
    "  return Sequential([Dense(20, input_dim=8, activation='relu'),\n",
    "                    Dense(20, activation='relu'),\n",
    "                    Dense(1, activation='linear')]) # Keep the output layer as linear because this is a regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8445606-05e4-4ce7-b3f5-3257852eaa37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1. Importing Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "```\n",
    "\n",
    "- **`import numpy as np`**: This line imports the NumPy library, which is a fundamental package for scientific computing in Python. It provides support for arrays, matrices, and a wide range of mathematical functions.\n",
    "\n",
    "- **`np.random.seed(0)`**: Setting the random seed ensures that the random numbers generated by NumPy will be the same each time you run the code. This is crucial for reproducibility. When working with machine learning models, it’s important to be able to reproduce results, and using a fixed seed helps achieve that.\n",
    "\n",
    "### 2. Importing TensorFlow and Setting the Seed\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)  # For reproducibility\n",
    "```\n",
    "\n",
    "- **`import tensorflow as tf`**: This line imports TensorFlow, which is a comprehensive library for building and training machine learning models, especially deep learning models.\n",
    "\n",
    "- **`tf.set_random_seed(42)`**: Similar to NumPy’s random seed, this sets the random seed for TensorFlow operations. Using a fixed value ensures that the initialization of weights and other random processes within TensorFlow will be the same across different runs, which is vital for consistent results.\n",
    "\n",
    "### 3. Importing Keras Modules\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "```\n",
    "\n",
    "- **`from tensorflow import keras`**: This imports the Keras API from TensorFlow. Keras is a high-level neural networks API that simplifies the process of building deep learning models. It provides an easier interface for creating and training neural networks compared to using TensorFlow directly.\n",
    "\n",
    "- **`from tensorflow.keras.models import Sequential`**: This imports the **Sequential** model class. A Sequential model is a linear stack of layers, where you can easily add layers one after another. This is useful for simple feedforward neural networks.\n",
    "\n",
    "- **`from tensorflow.keras.layers import Dense`**: This imports the **Dense** layer class. A Dense layer is a fully connected layer in a neural network, where each neuron in the layer receives input from all neurons in the previous layer. It’s a core building block of neural networks.\n",
    "\n",
    "### 4. Building the Model Function\n",
    "\n",
    "```python\n",
    "def build_model():\n",
    "    return Sequential([\n",
    "        Dense(20, input_dim=8, activation='relu'),\n",
    "        Dense(20, activation='relu'),\n",
    "        Dense(1, activation='linear')  # Linear output layer for regression\n",
    "    ])\n",
    "```\n",
    "\n",
    "#### Function Definition: `build_model()`\n",
    "- **`def build_model():`**: This line defines a function called `build_model()`. This function, when called, will create and return a neural network model.\n",
    "\n",
    "#### Creating the Sequential Model\n",
    "- **`return Sequential([...])`**: This returns a Sequential model constructed from the list of layers defined within the brackets.\n",
    "\n",
    "#### Layers of the Model\n",
    "1. **First Dense Layer**: \n",
    "   ```python\n",
    "   Dense(20, input_dim=8, activation='relu')\n",
    "   ```\n",
    "   - **`Dense(20, ...)`**: This creates a Dense layer with 20 neurons.\n",
    "   - **`input_dim=8`**: This specifies that the input to this layer will have 8 features (dimensions). It’s the shape of the input data.\n",
    "   - **`activation='relu'`**: This sets the activation function for this layer to **ReLU** (Rectified Linear Unit). ReLU introduces non-linearity to the model, allowing it to learn complex patterns. It outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "2. **Second Dense Layer**: \n",
    "   ```python\n",
    "   Dense(20, activation='relu')\n",
    "   ```\n",
    "   - This creates another Dense layer with 20 neurons and uses the ReLU activation function. It doesn’t need an `input_dim` parameter because Keras automatically infers the input shape from the previous layer.\n",
    "\n",
    "3. **Output Dense Layer**: \n",
    "   ```python\n",
    "   Dense(1, activation='linear')\n",
    "   ```\n",
    "   - **`Dense(1, ...)`**: This creates the output layer with 1 neuron. This is appropriate for a regression task, where we want to predict a single continuous value (like a housing price).\n",
    "   - **`activation='linear'`**: This means there’s no activation function applied; the output can be any real number. This is suitable for regression tasks where the output is continuous.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This code block sets up the foundation for a deep learning model using TensorFlow and Keras. Here’s a quick summary of what we’ve done:\n",
    "\n",
    "- We imported necessary libraries and set random seeds for reproducibility.\n",
    "- We created a function (`build_model`) that defines a simple neural network architecture with:\n",
    "  - **Two hidden layers** with 20 neurons each using the ReLU activation function.\n",
    "  - **One output layer** with a single neuron using a linear activation function, suitable for regression tasks.\n",
    "\n",
    "The model can be trained on datasets with 8 features to predict a continuous target variable, such as housing prices or other numerical outcomes. This structure forms the backbone of the machine learning task you’re trying to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c31c95-eea4-4b5c-a32e-bae0b2c84f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Shard Data\n",
    "\n",
    "From the [Horovod docs](https://github.com/horovod/horovod/blob/master/docs/concepts.rst):\n",
    "\n",
    "Horovod core principles are based on the MPI concepts size, rank, local rank, allreduce, allgather, and broadcast. These are best explained by example. Say we launched a training script on 4 servers, each having 4 GPUs. If we launched one copy of the script per GPU:\n",
    "\n",
    "* Size would be the number of processes, in this case, 16.\n",
    "\n",
    "* Rank would be the unique process ID from 0 to 15 (size - 1).\n",
    "\n",
    "* Local rank would be the unique process ID within the server from 0 to 3.\n",
    "\n",
    "We need to shard our data across our processes.  **NOTE:** We are using a Pandas DataFrame for demo purposes. In the next notebook we will use Parquet files with Petastorm for better scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f958472b-4ba5-4dc3-b3ed-84bd1926b2c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2004014418861574>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalifornia_housing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfetch_california_housing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn.datasets.california_housing'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-2004014418861574>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcalifornia_housing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfetch_california_housing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrank\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sklearn.datasets.california_housing'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'sklearn.datasets.california_housing'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_dataset(rank=0, size=1):\n",
    "  scaler = StandardScaler()\n",
    "  cal_housing = fetch_california_housing(data_home=\"/dbfs/ml/\" + str(rank) + \"/\")\n",
    "  X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
    "                                                       cal_housing.target,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=1)\n",
    "  scaler.fit(X_train)\n",
    "  X_train = scaler.transform(X_train[rank::size])\n",
    "  y_train = y_train[rank::size]\n",
    "  X_test = scaler.transform(X_test[rank::size])\n",
    "  y_test = y_test[rank::size]\n",
    "  return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847b7777-0482-4bea-8d54-c20d2cda6cec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedc96bb-2bf8-4805-a9da-06f73bf56533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "\u001B[0;32m<command-2004014418861576>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhorovod\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mhvd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbackend\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrun_training_horovod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n\u001B[0;32m<command-2004014418861576>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0moptimizers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mhorovod\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mhvd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mbackend\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mK\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrun_training_horovod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'tensorflow'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from keras import backend as K\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n",
    "  # config = tf.ConfigProto()\n",
    "  # config.gpu_options.allow_growth = True\n",
    "  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "  # K.set_session(tf.Session(config=config))\n",
    "  print(f\"Rank is: {hvd.rank()}\")\n",
    "  print(f\"Size is: {hvd.size()}\")\n",
    "  \n",
    "  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "  \n",
    "  model = build_model()\n",
    "  \n",
    "  from tensorflow.keras import optimizers\n",
    "  # Horovod: adjust learning rate based on number of GPUs/CPUs.\n",
    "  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "  \n",
    "  # Horovod: add Horovod Distributed Optimizer.\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "  \n",
    "  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d05f35e6-6a72-4ee3-8e59-44015f0999cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test it out on just the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "440ef8f1-7b65-4199-ae89-d24e5d480c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a8be20-c71c-4aa0-8631-ce634d5d85ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Better Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51c3e36d-9f9f-480d-93d1-fd5b6d8ba0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n",
    "  # config = tf.ConfigProto()\n",
    "  # config.gpu_options.allow_growth = True\n",
    "  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "  # K.set_session(tf.Session(config=config))\n",
    "  \n",
    "  \n",
    "  \n",
    "  print(f\"Rank is: {hvd.rank()}\")\n",
    "  print(f\"Size is: {hvd.size()}\")\n",
    "  \n",
    "  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "  \n",
    "  model = build_model()\n",
    "  \n",
    "  from tensorflow.keras import optimizers\n",
    "  # Horovod: adjust learning rate based on number of GPUs.\n",
    "  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "  \n",
    "  # Horovod: add Horovod Distributed Optimizer.\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "  # Use the optimized FUSE Mount\n",
    "  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights.ckpt\"\n",
    "  \n",
    "  callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "    \n",
    "    # Reduce the learning rate if training plateaus.\n",
    "    ReduceLROnPlateau(patience=10, verbose=1)\n",
    "  ]\n",
    "  \n",
    "  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "  if hvd.rank() == 0:\n",
    "    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n",
    "  \n",
    "  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd9f180-0530-4eb1-b773-441c1f3708c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test it out on just the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4a9170-da6b-45b6-a09c-073210930087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8f0dd9-698b-4cd0-9b4e-0f261db8937c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run on all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82393b05-faa7-4dba-8e80-2ae384ca843a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=0)\n",
    "hr.run(run_training_horovod)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Horovod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
